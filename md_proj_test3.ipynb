{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cf39f5633e8ae6",
   "metadata": {},
   "source": [
    "블로그 리뷰 10개 샘플 가지고 시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94453037ba9dd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "with open('./data/lib1.txt', 'r', encoding='utf-8') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae38bef466bcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tab\n",
    "txt_clean = re.sub(r'\\s+', ' ', txt).strip()\n",
    "# remove NBSP\n",
    "txt_clean = txt_clean.replace('\\xa0', ' ')\n",
    "# remove invis char\n",
    "txt_clean = txt_clean.replace('\\u200b', '')\n",
    "# remove ZWSP\n",
    "txt_clean = re.sub(r'[\\u200B]', '', txt_clean)\n",
    "# print(txt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cdce2ca6341f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"WhitePeak/bert-base-cased-Korean-sentiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c816ef9a93a513f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# small model\n",
    "nlp = spacy.load(\"ko_core_news_sm\")\n",
    "# 데탑용 large model (nvidia-smi)\n",
    "# nlp = spacy.load(\"ko_core_news_lg\")\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")  # rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c2e50cc37acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c21a2ee070fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(l) for l in [sent.text for sent in doc.sents][:10]]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fb0523e4fd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to a list of reviews\n",
    "doc_sents = [sent.text for sent in doc.sents]\n",
    "sentiment_cat = [('positive' if classifier(sent)[0]['label'] == 'LABEL_1' else 'negative', sent)\n",
    "                 for sent in tqdm(doc_sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac18df02b49d441",
   "metadata": {},
   "source": [
    "블로그 내용 자체의 연과성과 문장 분리부터 좀 불안한 관계로 sentiment analysis또한 일관적이지가 않은 편임.\n",
    "sentiment analysis 사용 안하기로 판정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752f332e632f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_cat[:10] , sentiment_cat[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58172f67a6df0e6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "단순 count기반 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ef332d7832713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def preprocess_txt(corpus: str):\n",
    "    # norm=True applies orthographic normalization\n",
    "    # stem=True reduces to word stem\n",
    "    lemmas = [(token, pos) for token, pos in okt.pos(corpus, norm=True, stem=True)]\n",
    "    # 특수문자 제거\n",
    "    lemmas_clean = [(re.sub(r'[^가-힣a-zA-Z\\s]', '', t), p) for t, p in lemmas]\n",
    "    # 한개짜리 단어 제거거\n",
    "    lemmas_clean = [(t, p) for t, p in lemmas_clean if len(t) > 1]\n",
    "\n",
    "    return lemmas_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ea0742c3a2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_clean = preprocess_txt(txt_clean)\n",
    "lemmas_clean[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b610d96248b1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = Counter([l[1] for l in lemmas_clean])\n",
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ce254513d6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pos = [  # 문장성분 추려내기\n",
    "    'Noun',\n",
    "    'Verb',\n",
    "    'Adjective',\n",
    "    # 'Adverb',\n",
    "    'Hashtag',\n",
    "]\n",
    "lemmas_clean_fin = [kw for kw, pos in lemmas_clean\n",
    "                    if pos in use_pos]\n",
    "len(lemmas_clean_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1684f921d64d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [(*okt.pos(w)[0], c)\n",
    "             for w, c in Counter(lemmas_clean_fin).items()\n",
    "             if c > 1]  # 너무 적은 것들은 포함 안함.\n",
    "\n",
    "[(*okt.pos(w)[0], c)\n",
    " for w, c in Counter(lemmas_clean_fin).most_common(25)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570068f1a357042f",
   "metadata": {},
   "source": [
    "TF-IDF통해서 키워드 추출\n",
    "\n",
    "블로그 하나를 document로 정의\n",
    "\n",
    "블로그별 top n단어 추출해서 집합하기?\n",
    "\n",
    "```python\n",
    "target = '목표 단어'\n",
    "TF = Counter(blogs['blog A'])['target']  # target count for blog A\n",
    "IDF = log(len(blogs) / (1 + len([blog for blog in blogs if target in blog])))\n",
    "\n",
    "# tf-idf for blog A is:\n",
    "TF * TDF\n",
    "```\n",
    "\n",
    "나중에는 집합한 TF-IDF들에 대하여 또 TF-IDF를 계산해서 도서관별 돋보이는 keyword 찾을수도? (충분히 vary한다면 될 듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e69a591a5e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그동안 한것들 pipeline으로\n",
    "def preprocess_txt_ex(corpus: str):\n",
    "    lemmas = [(token, pos) for token, pos in okt.pos(corpus, norm=True, stem=True)]\n",
    "    lemmas_clean = [(re.sub(r'[^가-힣a-zA-Z\\s]', '', t), p) for t, p in lemmas]\n",
    "    lemmas_clean = [(t, p) for t, p in lemmas_clean if len(t) > 1]\n",
    "\n",
    "    use_pos = [\n",
    "        'Noun',\n",
    "        'Verb',\n",
    "        'Adjective',\n",
    "        'Hashtag',\n",
    "    ]\n",
    "    lemmas_clean = [kw for kw, pos in lemmas_clean\n",
    "                    if pos in use_pos]\n",
    "\n",
    "    word_counts = {w: c\n",
    "                   for w, c in Counter(lemmas_clean).items()\n",
    "                   if c > 1}\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca21b0c2e71dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF를 위한 블로그별 그룹화\n",
    "txt_clean_grouped = txt_clean.split('[STOP]')[:-1]  # [STOP] 기준 블로그 delimeter 쪼개기 후, 마지막 꼬투리 제거\n",
    "blogs_clean_grouped = [preprocess_txt_ex(c) for c in txt_clean_grouped]\n",
    "print('no. group:', len(blogs_clean_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5565ef3353bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에꺼들 pipeline으로 만들기\n",
    "# 블로그별 상위 n개만 저장\n",
    "tfidf_top = list()\n",
    "\n",
    "_word_list = list(set(  # 중복제거\n",
    "    [k for blog in blogs_clean_grouped for k, _ in blog.items()]\n",
    "))\n",
    "\n",
    "df_scores = [(w, len([True for blog in blogs_clean_grouped if w in blog])) for w in _word_list]\n",
    "\n",
    "doc_len = len(blogs_clean_grouped)\n",
    "idf_scores = [(w, math.log(doc_len / (1 + df)) ) for w, df in df_scores]\n",
    "\n",
    "total_tf_idf_top = list()\n",
    "for blog in blogs_clean_grouped:  # 각 document(blog)에 대한 TF-IDF점수를 구할 수 있음.\n",
    "    tf_scores = [(w, blog.get(w, 0)) for w in _word_list]\n",
    "    tfidf_scores = [(tf[0], tf[1] * idf[1]) for tf, idf in zip(tf_scores, idf_scores) if tf[1] > 0]\n",
    "\n",
    "    tfidf_scores_top = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:10]  # top 10\n",
    "    total_tf_idf_top += tfidf_scores_top\n",
    "\n",
    "total_tf_idf_top.sort(key=lambda x: x[1], reverse=True)\n",
    "[print(l) for l in total_tf_idf_top[:25]]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164ded509db0651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
